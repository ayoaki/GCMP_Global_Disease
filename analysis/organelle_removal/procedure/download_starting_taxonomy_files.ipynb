{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Starting Taxonomy Files\n",
    "\n",
    "In order to remove coral mitochondria, we need to first build supplemented versions of the greengenes and SILVA reference databases. In this notebook we automated downloading and extracting the databases.\n",
    "\n",
    "NOTE: these references are large, so it will take a while for them to download, and you should expect them to occupy several Gb of hard drive space. (I'd hesitate before running if < 15Gb are free on your harddrive).\n",
    "\n",
    "gg_13_8_otus.tar.gz (from ftp://greengenes.microbio.me/greengenes_release/gg_13_5/gg_13_8_otus.tar.gz)  \n",
    "Silva_132_release.zip (from https://www.arb-silva.de/fileadmin/silva_databases/qiime/Silva_132_release.zip)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up utility functions for downloading data and organizing our folders\n",
    "\n",
    "import urllib.request\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "\n",
    "def download_file(url, local_filepath):\n",
    "    \"\"\"Download a file from a remote url and save to a local filepath\n",
    "    \n",
    "    url - the web address of the file you want to download as a string\n",
    "    local_filepath - the local filepath to which the file will be saved\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Downloading file: {url}\")\n",
    "    # This is slightly convoluted-looking, but we are getting a response from the webpage and\n",
    "    # then copying that to the file. \n",
    "\n",
    "    #Hat-tip to stack overflow: \n",
    "    #https://stackoverflow.com/questions/7243750/download-file-from-web-in-python-3\n",
    "\n",
    "    with urllib.request.urlopen(url) as response, open(local_filepath, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    "    \n",
    "    print(f\"Saved to local filepath: {local_filepath}\")\n",
    "    \n",
    "def make_directory(path):\n",
    "    \"\"\"Make a directory, but proceed without errors if it fails\n",
    "    path -- the path to the directory (e.g. \"../output/taxonomy_references\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError:\n",
    "        print (f\"Creation of directory {path} failed\")\n",
    "    else:\n",
    "        print (f\"Created the directory {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepaths and urls\n",
    "\n",
    "gg_url = \"ftp://greengenes.microbio.me/greengenes_release/gg_13_5/gg_13_8_otus.tar.gz\"\n",
    "data_folder = \"../output/taxonomy_references/\"\n",
    "local_gg_filename = \"gg_13_8_otus.tar.gz\"\n",
    "\n",
    "silva_url = \" https://www.arb-silva.de/fileadmin/silva_databases/qiime/Silva_132_release.zip\"\n",
    "local_silva_filename = \"Silva_132_release.zip\"\n",
    "local_silva_filepath = os.path.join(data_folder,local_silva_filename)\n",
    "local_gg_filepath = os.path.join(data_folder,local_gg_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of directory ../output/taxonomy_references/ failed\n"
     ]
    }
   ],
   "source": [
    "#### Set up a folder to hold large taxonomy files\n",
    "import os\n",
    "\n",
    "# create the data folder if it doesn't already exist\n",
    "make_directory(data_folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the Greengenes Taxonomy\n",
    "\n",
    "We'll now download the greengenes 13_8 taxonomy reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file: ftp://greengenes.microbio.me/greengenes_release/gg_13_5/gg_13_8_otus.tar.gz\n",
      "Saved to local filepath: ../output/taxonomy_references/gg_13_8_otus.tar.gz\n"
     ]
    }
   ],
   "source": [
    "download_file(url=gg_url,local_filepath = local_gg_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to expand the greengenes .tar.gz file into our input folder so we can access the contents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "tar = tarfile.open(local_gg_filepath, \"r:gz\")\n",
    "tar.extractall(path=data_folder)\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download and Expand the SILVA 132 release\n",
    "\n",
    "Now we'll download the SIVLA 132 release and decompress it.\n",
    "NOTE: this is a large (~2.47 Gb) file, so it may take a while to download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading file:  https://www.arb-silva.de/fileadmin/silva_databases/qiime/Silva_132_release.zip\n",
      "Saved to local filepath: ../output/taxonomy_references/Silva_132_release.zip\n"
     ]
    }
   ],
   "source": [
    "download_file(url=silva_url,local_filepath = local_silva_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import is_zipfile, ZipFile\n",
    "\n",
    "if not is_zipfile(local_silva_filepath):\n",
    "    raise ValueError(\"The SILVA database zip file {local_silva_filepath} doesn't look like a zip file. Was it downloaded correctly?\")\n",
    "\n",
    "silva_zipfile = ZipFile(local_silva_filepath)\n",
    "\n",
    "#Obnoxiously this file contains a _MACOSX subfolder. We don't want to unzip that...\n",
    "files_to_extract = [m for m in silva_zipfile.namelist() if \"_MACOSX\" not in m]\n",
    "print(\"Extracting SILVA database...\")\n",
    "silva_zipfile.extractall(path = data_folder,members = files_to_extract)\n",
    "silva_zipfile.close()\n",
    "print(f\"Extracted the SILVA 132 database into: {data_folder}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
